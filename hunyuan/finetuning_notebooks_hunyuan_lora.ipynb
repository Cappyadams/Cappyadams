{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ“Œ **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.0\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown To train Hunyuan Video lora 24GB VRAM is recommended.\n",
        "#@markdown  <br>If your dataset contains videos, then more than 24GB is recommended.\n",
        "#@markdown <br>You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form",
        "outputId": "a5867c83-6ddd-427e-bb10-0aa687276091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 18 16:50:19 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form",
        "outputId": "7a374e4d-dbf7-4d20-dc49-b81b1ba66e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion-pipe'...\n",
            "remote: Enumerating objects: 715, done.\u001b[K\n",
            "remote: Counting objects: 100% (272/272), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 715 (delta 243), reused 235 (delta 217), pack-reused 443 (from 1)\u001b[K\n",
            "Receiving objects: 100% (715/715), 7.80 MiB | 14.68 MiB/s, done.\n",
            "Resolving deltas: 100% (444/444), done.\n",
            "Submodule 'submodules/Cosmos' (https://github.com/NVIDIA/Cosmos) registered for path 'submodules/Cosmos'\n",
            "Submodule 'submodules/HunyuanVideo' (https://github.com/Tencent/HunyuanVideo) registered for path 'submodules/HunyuanVideo'\n",
            "Submodule 'submodules/Lumina_2' (https://github.com/Alpha-VLLM/Lumina-Image-2.0) registered for path 'submodules/Lumina_2'\n",
            "Cloning into '/content/diffusion-pipe/submodules/Cosmos'...\n",
            "remote: Enumerating objects: 392, done.        \n",
            "remote: Counting objects: 100% (115/115), done.        \n",
            "remote: Compressing objects: 100% (81/81), done.        \n",
            "remote: Total 392 (delta 63), reused 34 (delta 34), pack-reused 277 (from 1)        \n",
            "Receiving objects: 100% (392/392), 15.83 MiB | 16.51 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n",
            "Cloning into '/content/diffusion-pipe/submodules/HunyuanVideo'...\n",
            "remote: Enumerating objects: 771, done.        \n",
            "remote: Counting objects: 100% (479/479), done.        \n",
            "remote: Compressing objects: 100% (122/122), done.        \n",
            "remote: Total 771 (delta 420), reused 357 (delta 357), pack-reused 292 (from 1)        \n",
            "Receiving objects: 100% (771/771), 72.89 MiB | 16.60 MiB/s, done.\n",
            "Resolving deltas: 100% (475/475), done.\n",
            "Cloning into '/content/diffusion-pipe/submodules/Lumina_2'...\n",
            "remote: Enumerating objects: 131, done.        \n",
            "remote: Counting objects: 100% (131/131), done.        \n",
            "remote: Compressing objects: 100% (125/125), done.        \n",
            "remote: Total 131 (delta 53), reused 33 (delta 1), pack-reused 0 (from 0)        \n",
            "Receiving objects: 100% (131/131), 69.64 MiB | 1.91 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Submodule path 'submodules/Cosmos': checked out 'a6e2fdd49053ae75836cedc2a99c7c84bc1c8c1b'\n",
            "Submodule path 'submodules/HunyuanVideo': checked out 'c4a9d7708dac7c930181c9e147d0092dffa36f92'\n",
            "Submodule path 'submodules/Lumina_2': checked out '09362957c2ce37407c7982fea742a8a72686b882'\n",
            "/content/diffusion-pipe\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.3.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->deepspeed)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->deepspeed)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->deepspeed)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.3-py3-none-any.whl size=1550058 sha256=f330ceacd409658bc94da6baa29398f9f75c92f27a557944ea4162f3f9a079b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/dc/d4/7e7e07b11bc7c0e2a1a495b967acf58de61261eed4596fb23b\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, hjson, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepspeed\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deepspeed-0.16.3 hjson-3.1.0 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting torch-optimi\n",
            "  Downloading torch_optimi-0.2.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from torch-optimi) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from torch-optimi) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torch-optimi) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->torch-optimi) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->torch-optimi) (3.0.2)\n",
            "Downloading torch_optimi-0.2.1-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: torch-optimi\n",
            "Successfully installed torch-optimi-0.2.1\n"
          ]
        }
      ],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/tdrussell/diffusion-pipe\n",
        "!git clone --recurse-submodules https://github.com/tdrussell/diffusion-pipe\n",
        "%cd diffusion-pipe\n",
        "\n",
        "# Cherry picked dependencies to use in Colab.\n",
        "!pip install deepspeed\n",
        "!pip install datasets\n",
        "!pip install torch-optimi\n",
        "!pip install bitsandbytes\n",
        "!pip install av\n",
        "!pip install loguru\n",
        "!pip install flash-attn\n",
        "\n",
        "\n",
        "# Comment on the requirements above, and uncomment below if you're not using Colab.\n",
        "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install deepspeed\n",
        "# !pip install toml\n",
        "# !pip install transformers\n",
        "# !pip install diffusers>=0.32.1\n",
        "# !pip install datasets\n",
        "# !pip install pillow\n",
        "# !pip install sentencepiece\n",
        "# !pip install protobuf\n",
        "# !pip install peft\n",
        "# !pip install torch-optimi\n",
        "# !pip install tensorboard\n",
        "# !pip install tqdm\n",
        "# !pip install safetensors\n",
        "# !pip install bitsandbytes\n",
        "# !pip install imageio[ffmpeg]\n",
        "# !pip install av\n",
        "# !pip install einops\n",
        "# !pip install accelerate\n",
        "# !pip install loguru\n",
        "# !pip install flash-attn; sys_platform==linux\n",
        "# !pip install omegaconf\n",
        "# !pip install iopath\n",
        "# !pip install termcolor\n",
        "# !pip install hydra-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training dataset.\n",
        "\n",
        "#@markdown When training Hunyuan Lora, the dataset could contatin both images and videos.\n",
        "\n",
        "#@markdown Each file should have a corresponding text file (`.txt`) with the same name. <br>\n",
        "#@markdown **Each video must have a specific number of frames, as much as you will define later in \"frame_buckets\".**\n",
        "\n",
        "#@markdown The text file contains prompts associated with the video or image.\n",
        "\n",
        "\n",
        "#@markdown ### Example Dataset Structure:\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown â”œâ”€â”€ a (1).mp4         # Video file\n",
        "#@markdown â”œâ”€â”€ a (1).txt         # Corresponding prompt for a (1).mp4\n",
        "#@markdown â”œâ”€â”€ a (2).mp4         # Another video file\n",
        "#@markdown â”œâ”€â”€ a (2).txt         # Corresponding prompt for a (2).mp4\n",
        "#@markdown â”œâ”€â”€ a (3).png         # Image file\n",
        "#@markdown â”œâ”€â”€ a (3).txt         # Corresponding prompt for a (3).png\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown This cell will download base models. If you don't already have the base model files in your google drive, run this.\n",
        "\n",
        "#@markdown You need Huggingface token (Read permission) to run this.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://media.githubusercontent.com/media/jhj0517/finetuning-notebooks/master/docs/screenshots/colab_secrets.png)\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "# Set params\n",
        "BASE_MODELS_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/hunyuan/base_models\" # @param {type:\"string\"}\n",
        "HUNYUAN_VIDEO = \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\" #@param [\"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\", \"hunyuan_video_720_cfgdistill_bf16.safetensors\", \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\"]\n",
        "VAE = \"hunyuan_video_vae_bf16.safetensors\" #@param [\"hunyuan_video_vae_bf16.safetensors\", \"hunyuan_video_vae_fp32.safetensors\"]\n",
        "CLIP = \"openai/clip-vit-large-patch14\" #@param [\"openai/clip-vit-large-patch14\"]\n",
        "LLM = \"Kijai/llava-llama-3-8b-text-encoder-tokenizer\" #@param [\"Kijai/llava-llama-3-8b-text-encoder-tokenizer\"]\n",
        "\n",
        "#@markdown Models will be downloaded from\n",
        "#@markdown - hunyuan: https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main\n",
        "#@markdown - clip: https://huggingface.co/openai/clip-vit-large-patch14\n",
        "#@markdown - llm: https://huggingface.co/Kijai/llava-llama-3-8b-text-encoder-tokenizer\n",
        "\n",
        "\n",
        "# Initialize dir paths\n",
        "HUNYUAN_MODELS_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "os.makedirs(HUNYUAN_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "CLIP_MODEL_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"clip\")\n",
        "os.makedirs(CLIP_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "LLM_MODEL_DIR = os.path.join(BASE_MODELS_DIR_PATH, \"llm\")\n",
        "os.makedirs(LLM_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "HUNYUAN_VIDEO_URL = {\n",
        "    \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\",\n",
        "    \"hunyuan_video_720_cfgdistill_bf16.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_720_cfgdistill_bf16.safetensors\",\n",
        "    \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\",\n",
        "}\n",
        "\n",
        "HUNYUAN_VIDEO_VAE_URL = {\n",
        "    \"hunyuan_video_vae_bf16.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_vae_bf16.safetensors\",\n",
        "    \"hunyuan_video_vae_fp32.safetensors\": \"https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_vae_fp32.safetensors\",\n",
        "}\n",
        "\n",
        "# Download models\n",
        "video_url, vae_url = HUNYUAN_VIDEO_URL[HUNYUAN_VIDEO], HUNYUAN_VIDEO_VAE_URL[VAE]\n",
        "clip_repo_id, llm_repo_id = CLIP, LLM\n",
        "\n",
        "!wget {video_url} -P {HUNYUAN_MODELS_DIR}\n",
        "!wget {vae_url} -P {HUNYUAN_MODELS_DIR}\n",
        "\n",
        "huggingface_hub.snapshot_download(\n",
        "    clip_repo_id,\n",
        "    local_dir = CLIP_MODEL_DIR,\n",
        ")\n",
        "huggingface_hub.snapshot_download(\n",
        "    llm_repo_id,\n",
        "    local_dir = LLM_MODEL_DIR,\n",
        ")\n"
      ],
      "metadata": {
        "id": "9WzQRwZij5jf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import sys\n",
        "import toml\n",
        "\n",
        "#@markdown If you're intended to train Lora from previous checkpoint, check this.\n",
        "RESUME_FROM_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## Paths Configuration\n",
        "OUTPUT_LORA_NAME = \"My-Hunyuan-Lora-V1\" # @param {type:\"string\"}\n",
        "OUTPUT_LORA_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/hunyuan/outputs\"  # @param {type:\"string\"}\n",
        "BASE_MODELS_DIR_PATH = \"/content/drive/MyDrive/finetuning-notebooks/hunyuan/base_models\" # @param {type:\"string\"}\n",
        "DATASET_PATH = \"/content/drive/MyDrive/finetuning-notebooks/dataset/dog\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "OUTPUT_LORA_DIR_PATH = os.path.join(OUTPUT_LORA_DIR_PATH, OUTPUT_LORA_NAME)\n",
        "transformer_dir_path = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "vae_dir_path = os.path.join(BASE_MODELS_DIR_PATH, \"hunyuan\")\n",
        "clip_path = os.path.join(BASE_MODELS_DIR_PATH, \"clip\")\n",
        "llm_path = os.path.join(BASE_MODELS_DIR_PATH, \"llm\")\n",
        "\n",
        "#@markdown ## Dataset Configuration\n",
        "#@markdown - **`frame_buckets`** is the list of frame numbers in your dataset.\n",
        "#@markdown <br>For example, if your dataset contains 30, 60 frames of videos, then use : [30, 60]\n",
        "#@markdown <br>Don't use too long frames unless you don't have a lot of VRAM.\n",
        "#@markdown <br>If your dataset also contains images, then use : [1, 30, 60]\n",
        "#@markdown - **`resolutions`** is the list of resolutions which **`diffusion-pipe`** will resize your dataset.\n",
        "#@markdown <br>**`diffusion-pipe`** is smart to handle resizing your dataset by 1:2 or 2:1 image etc.\n",
        "#@markdown <br>If you have less than 24GB of VRAM, just set it to 512, then increase it according to your device.\n",
        "## Frame Buckets Settings\n",
        "frame_buckets = [1]  # @param {type:\"raw\"}\n",
        "# You can use 1024 if you have 24 GB > VRAM.\n",
        "resolutions = [512]  # @param {type:\"raw\"}\n",
        "## Aspect Ratio Bucketing Settings\n",
        "enable_ar_bucket = True  # @param {type:\"boolean\"}\n",
        "min_ar = 0.5  # @param {type:\"number\"}\n",
        "max_ar = 2.0  # @param {type:\"number\"}\n",
        "num_ar_buckets = 7  # @param {type:\"integer\"}\n",
        "# Reduce as necessary\n",
        "num_repeats = 5\n",
        "\n",
        "# Write dataset.toml\n",
        "dataset_config = {\n",
        "    \"resolutions\": resolutions,\n",
        "    \"frame_buckets\": frame_buckets,\n",
        "\n",
        "    \"enable_ar_bucket\": enable_ar_bucket,\n",
        "    \"min_ar\": min_ar,\n",
        "    \"max_ar\": max_ar,\n",
        "    \"num_ar_buckets\": num_ar_buckets,\n",
        "    \"directory\": [\n",
        "        {\n",
        "            \"path\": DATASET_PATH,\n",
        "            \"num_repeats\": num_repeats,\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "os.makedirs(OUTPUT_LORA_DIR_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "dataset_config_file_path = os.path.join(DATASET_PATH, \"dataset.toml\")\n",
        "with open(dataset_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(dataset_config, toml_file)\n",
        "print(f\"dataset.toml is saved to {dataset_config_file_path}\")\n",
        "\n",
        "#@markdown ## Base Model Configuration\n",
        "BASE_HUNYUAN_MODEL_NAME = \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\" #@param [\"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\", \"hunyuan_video_720_cfgdistill_bf16.safetensors\", \"hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\"]\n",
        "BASE_VAE_MODEL_NAME = \"hunyuan_video_vae_bf16.safetensors\" #@param [\"hunyuan_video_vae_bf16.safetensors\", \"hunyuan_video_vae_fp32.safetensors\"]\n",
        "\n",
        "transformer_path = os.path.join(transformer_dir_path, BASE_HUNYUAN_MODEL_NAME)\n",
        "vae_path = os.path.join(vae_dir_path, BASE_VAE_MODEL_NAME)\n",
        "\n",
        "model_type = 'hunyuan-video'\n",
        "dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "transformer_dtype = 'float8'  # @param {type:\"string\"}\n",
        "timestep_sample_method = 'logit_normal'  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Training Settings\n",
        "epochs = 50  # @param {type:\"integer\"}\n",
        "# Batch size of a single forward/backward pass for one GPU.\n",
        "micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "# Pipeline parallelism degree. A single instance of the model is divided across this many GPUs.\n",
        "pipeline_stages = 1  # @param {type:\"integer\"}\n",
        "gradient_accumulation_steps = 4  # @param {type:\"integer\"}\n",
        "gradient_clipping = 1.0  # @param {type:\"number\"}\n",
        "# Learning rate warmup.\n",
        "warmup_steps = 50  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Eval Settings\n",
        "eval_every_n_epochs = 1  # @param {type:\"integer\"}\n",
        "eval_before_first_step = True  # @param {type:\"boolean\"}\n",
        "eval_micro_batch_size_per_gpu = 1  # @param {type:\"integer\"}\n",
        "eval_gradient_accumulation_steps = 1  # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Lora Settings\n",
        "adapter_type = 'lora'\n",
        "rank = 32  # @param {type:\"integer\"}\n",
        "adapter_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "\n",
        "# Optimizer settings\n",
        "optimizer_type = 'adamw_optimi'  # @param {type:\"string\"}\n",
        "lr = 2e-5  # @param {type:\"number\"}\n",
        "betas = [0.9, 0.99]  # @param {type:\"raw\"}\n",
        "weight_decay = 0.02  # @param {type:\"number\"}\n",
        "eps = 1e-8  # @param {type:\"number\"}\n",
        "\n",
        "#@markdown ## Misc Settings\n",
        "save_every_n_epochs = 10  # @param {type:\"integer\"}\n",
        "checkpoint_every_n_minutes = 30  # @param {type:\"integer\"}\n",
        "activation_checkpointing = True  # @param {type:\"boolean\"}\n",
        "partition_method = 'parameters'  # @param {type:\"string\"}\n",
        "save_dtype = 'bfloat16'  # @param {type:\"string\"}\n",
        "caching_batch_size = 1  # @param {type:\"integer\"}\n",
        "steps_per_print = 1  # @param {type:\"integer\"}\n",
        "video_clip_mode = 'single_middle'  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Write config.toml\n",
        "train_config = {\n",
        "    \"output_dir\": OUTPUT_LORA_DIR_PATH,\n",
        "    \"dataset\": dataset_config_file_path,\n",
        "\n",
        "    # Training Settings\n",
        "    \"epochs\": epochs,\n",
        "    \"micro_batch_size_per_gpu\": micro_batch_size_per_gpu,\n",
        "    \"pipeline_stages\": pipeline_stages,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"gradient_clipping\": gradient_clipping,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "\n",
        "    # Eval Settings\n",
        "    \"eval_every_n_epochs\": eval_every_n_epochs,\n",
        "    \"eval_before_first_step\": eval_before_first_step,\n",
        "    \"eval_micro_batch_size_per_gpu\": eval_micro_batch_size_per_gpu,\n",
        "    \"eval_gradient_accumulation_steps\": eval_gradient_accumulation_steps,\n",
        "\n",
        "    # Misc Settings\n",
        "    \"save_every_n_epochs\": save_every_n_epochs,\n",
        "    \"checkpoint_every_n_minutes\": checkpoint_every_n_minutes,\n",
        "    \"activation_checkpointing\": activation_checkpointing,\n",
        "    \"partition_method\": partition_method,\n",
        "    \"save_dtype\": save_dtype,\n",
        "    \"caching_batch_size\": caching_batch_size,\n",
        "    \"steps_per_print\": steps_per_print,\n",
        "    \"video_clip_mode\": video_clip_mode,\n",
        "\n",
        "    \"model\": {\n",
        "        \"type\": model_type,\n",
        "        \"transformer_path\": transformer_path,\n",
        "        \"vae_path\": vae_path,\n",
        "        \"llm_path\": llm_path,\n",
        "        \"clip_path\": clip_path,\n",
        "        \"dtype\": dtype,\n",
        "        \"transformer_dtype\": transformer_dtype,\n",
        "        \"timestep_sample_method\": timestep_sample_method,\n",
        "    },\n",
        "\n",
        "    \"adapter\": {\n",
        "        \"type\": \"lora\",\n",
        "        \"rank\": rank,\n",
        "        \"dtype\": adapter_dtype,\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": optimizer_type,\n",
        "        \"lr\": lr,\n",
        "        \"betas\": betas,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"eps\": eps,\n",
        "    },\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "train_config_file_path = os.path.join(DATASET_PATH, \"config.toml\")\n",
        "with open(train_config_file_path, \"w\") as toml_file:\n",
        "    toml.dump(train_config, toml_file)\n",
        "print(f\"config.toml is saved to {train_config_file_path}\")\n",
        "\n",
        "\n",
        "## Train\n",
        "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
        "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path} --resume_from_checkpoint\n",
        "else:\n",
        "  !deepspeed --num_gpus=1 train.py --deepspeed --config {train_config_file_path}\n"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}